REIMPLEMENATION OF FILET USING DEEP NEURAL NET

The original FILET approach (BELOW) used a tree based method to detect introgression.  It relied on a user specified feature vector based pop gen summary stats. The advantage of using a neural network is that the network itself extracts a feature vector during training, meaning it's not limited by the current the state of the art in pop gen summary stats.

My first attempt involved using a convolutional neural network architecture.  First I encoded the training data (bi-allelic SNPs encoded as binary matrices) as a sort of binary image. An example image can be found at the link below.

https://github.com/flag0010/FILET/blob/master/Figure_1.png

There are 34 lines, 20 from pop1 and 14 from pop2. Normally SNP matrices are displayed with individuals on rows and SNPs (segregating sites) on cols, but I transposed these SNP matrices for reasons I'll explain below.  Thus the image and SNP matrix are 1102x34. 

Next I had to deal with the fact that the training SNP matrices are of variable length.  In the training data supplied with FILET I found the longest had 1102 SNPs. I padded all training data to this length by adding zeros.  This is why the the bottom of the matrix image above is all black.

Now I'm fiddling with network architecture.  Because the SNP matrices are transposed a 1D convolution (row-wise) will stride across a single SNP for all 34 individuals.  This allows the neural net to do something like image segmentation and discover that there are 20 indivuals from pop1 and 14 from pop2 (the network is not aware of pop labels).  I'm still working on optimizing this striding along with pooling to 1) keep the network relatively small, yet 2) allow it to efficiently discover populations and extract meaningful features.  

Finally, as with the original version, the output classification includes 3 possibilities.  Migration from pop1 into pop2, the reverse, and no migration. This is accomplished in the standard way using a final softmax network layer with 3 outputs, one for each class.

Two other observations:
1) one can shuffle the rows (SNPs) in the SNP arrays to create new inputs using existing inputs.  This should help in training the network, much in the same way that people add perturbations to images when training image classifiers
2) one can flip all the bits in the binary SNP matrix (e.g. for matrix m, flipped = m*-1+1). This also can be used to create new inputs, and trains the network not to focus on specific values (0 or 1), but rather on patterns and correlations between values. This seems important since assigning SNPs to bits is arbitrary.

to run, first install python 2.7, with numpy, keras and theano.  Then extract the data running:
python extract.data.set.py

Then fit a simple (and non-optimal) CNN:
python fit.theano.60.py

The CNN fits for 60 epochs and has an architecture as follows:

Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_5 (Conv1D)            (None, 1101, 128)         8832      
_________________________________________________________________
conv1d_6 (Conv1D)            (None, 1100, 64)          16448     
_________________________________________________________________
max_pooling1d_4 (MaxPooling1 (None, 550, 64)           0         
_________________________________________________________________
conv1d_7 (Conv1D)            (None, 549, 64)           8256      
_________________________________________________________________
max_pooling1d_5 (MaxPooling1 (None, 274, 64)           0         
_________________________________________________________________
conv1d_8 (Conv1D)            (None, 273, 64)           8256      
_________________________________________________________________
max_pooling1d_6 (MaxPooling1 (None, 136, 64)           0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 8704)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               1114240   
_________________________________________________________________
dense_3 (Dense)              (None, 3)                 387       
=================================================================
Total params: 1,156,419
Trainable params: 1,156,419
Non-trainable params: 0
_________________________________________________________________


Short runs seem to suggest a test set accuracy of classification of about 85% is possible.  Perhaps better with tuning.  The goal is to get this > 95%.












This directory contains all of the code necessary to run FILET (Finding Introgressed Loci using Extra-Trees). Briefly, this tool uses an Extra-Trees classifier (Geurts et al. 2005: http://www.montefiore.ulg.ac.be/~ernst/uploads/news/id63/extremely-randomized-trees.pdf) to classify genomic windows from a as one of three different modes of evolution based on population genomic data from a phased two-species sample:
1) Introgression from species 1 to species 2
2) Introgression from species 2 to species 1
3) No introgression

The manuscript describing FILET will be posted on biorxiv shortly, at which point I will add the link to this README. Meanwhile, you can check out this talk for a brief overview of the method and results from an application to Drosophila simulans and D. sechellia: https://www.youtube.com/watch?v=t6zy8ZsRKt4

In order to run FILET, the user will need several python packages, including scikit-learn (http://scikit-learn.org/stable/), scipy (http://www.scipy.org/), and numpy (http://www.numpy.org/). The simplest way to get all of this up and running is to install Anaconda (https://store.continuum.io/cshop/anaconda/), which contains these and many more python packages and is very easy to install. You will also need GCC (https://gcc.gnu.org/) and GSL (https://www.gnu.org/software/gsl/). The FILET pipeline should then work on any Linux system (and probably OS X too, though this has not yet been tested).

Once you have cloned this repository into the desired directory, cd into this directory and then run:

make all

This will compile the C tools included in this pipeline, and you should then be ready to rock. However, because FILET requires training prior to performing classifications, running the software is a multi-step process. Below, we outline each step, and in examplePipeline.sh we describe the whole process in greater detail, with a complete example run. Here are the steps of the FILET pipeline:

- Simulate training data: Statistics summarizing patterns of variation within each simulated genomic window must be calculated from training data.
- Calculate feature vectors from these training data, and combine into a matrix of labeled training examples.
- Train the FILET classifier.
- Calculate summary statistics from genomic windows we wish to classify.
- Classify genomic data (with known or inferred haplotypic phase).

Not all of these steps are mandatory: if you have generated your own feature vectors from training data, then provided these data are in the proper format (see example in featureVectorsToClassify/) you can proceed to the training step. Similarly, if you have calculated summary statistics from genomic data, then once a classifier has been trained you can skip straight to the data classification step as well (again, provided these statistics are listed in the proper format; see example in dataToClassify/). Note that this pipeline also assumes that you have already simulated training data (with ms-style output), which can be done using msmove (https://github.com/geneva/msmove) or the simulator of your choice. The requirements for simulated data and their file names are described in the comments above Step 4 of examplePipeline.sh.

Note: the full results from our scan in D. simulans and D. sechellia will be added to a directory called simSechResults later today. (I.e. once my dev server comes back up...)
